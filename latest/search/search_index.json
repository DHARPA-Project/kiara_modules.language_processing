{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"kiara plugin: language_processing","text":"<p>This package contains a set of commonly used/useful modules, pipelines, types and metadata schemas for Kiara.</p>"},{"location":"#description","title":"Description","text":"<p>Language-processing kiara modules and data types.</p>"},{"location":"#package-content","title":"Package content","text":""},{"location":"#module_types","title":"module_types","text":"<ul> <li> <p><code>generate.LDA.for.tokens_array</code>: Perform Latent Dirichlet Allocation on a tokenized corpus.</p> </li> <li> <p><code>tokenize.string</code>: Tokenize a string.</p> </li> <li> <p><code>tokenize.texts_array</code>: Split sentences into words or words into characters.</p> </li> <li> <p><code>create.stopwords_list</code>: Create a list of stopwords from one or multiple sources.</p> </li> <li> <p><code>remove_stopwords.from.tokens_array</code>: Remove stopwords from an array of token-lists.</p> </li> <li> <p><code>preprocess.tokens_array</code>: Preprocess lists of tokens, incl. lowercasing, remove special characers, etc.</p> </li> </ul>"},{"location":"#operations","title":"operations","text":"<ul> <li> <p><code>create.stopwords_list</code>: Create a list of stopwords from one or multiple sources.</p> </li> <li> <p><code>generate.LDA.for.tokens_array</code>: Perform Latent Dirichlet Allocation on a tokenized corpus.</p> </li> <li> <p><code>preprocess.tokens_array</code>: Preprocess lists of tokens, incl. lowercasing, remove special characers, etc.</p> </li> <li> <p><code>remove_stopwords.from.tokens_array</code>: Remove stopwords from an array of token-lists.</p> </li> <li> <p><code>tokenize.string</code>: Tokenize a string.</p> </li> <li> <p><code>tokenize.texts_array</code>: Split sentences into words or words into characters.</p> </li> </ul>"},{"location":"#links","title":"Links","text":"<ul> <li>Documentation: https://DHARPA-Project.github.io/kiara_plugin.language_processing</li> <li>Code: https://github.com/DHARPA-Project/kiara_plugin.language_processing</li> </ul>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Home</li> <li>Package contents</li> <li>Usage</li> <li>Development</li> <li>API reference</li> </ul>"},{"location":"development/","title":"Development","text":""},{"location":"development/#prepare-development-environment","title":"Prepare development environment","text":""},{"location":"development/#using-conda-recommended","title":"Using conda (recommended)","text":"<pre><code>conda create -n language_processing python=3.9\nconda activate language_processing\nconda install -c conda-forge mamba   # this is optional, but makes everything install related much faster, if you don't use it, replace 'mamba' with 'conda' below\nmamba install -c conda-forge -c dharpa kiara\nmamba install -c conda-forge -c dharpa kiara_plugin.core_types kiara_plugin.tabular   # optional, adjust which plugin packages you depend on, those two are quite common\n</code></pre>"},{"location":"development/#using-python-venv","title":"Using Python venv","text":"<p>Later, alligator.</p>"},{"location":"development/#check-out-the-source-code","title":"Check out the source code","text":"<p>First, fork the kiara_plugin.language_processing repository into your personal Github account.</p> <p>Then, use the resulting url (in my case: https://github.com/makkus/kiara_modules.language_processing.git) to clone the repository locally:</p> <pre><code>https://github.com/&lt;YOUR_FORKED_GITHUB_ID&gt;/kiara_plugin.language_processing\n</code></pre>"},{"location":"development/#install-the-kiara-plugin-package-into-it","title":"Install the kiara plugin package into it","text":"<pre><code>cd kiara_plugin.language_processing\npip install -e '.[all_dev]'\n</code></pre> <p>Here we use the <code>-e</code> option for the <code>pip install</code> command. This installs the local folder as a package in development mode into the current environment. Development mode makes it so that if you change any of the files in this folder, the Python environment will pick it up automatically, and whenever you run anything in this environment the latest version of your code/files are used.</p> <p>We also install a few additional requirements  (the <code>[all_dev]</code> part in the command above) that are not strictly necessary for <code>kiara</code> itself, or this package, but help with various development-related tasks.</p>"},{"location":"development/#install-some-pre-commit-check-tooling-optional","title":"Install some pre-commit check tooling (optional)","text":"<p>This step is optional, but helps with keeping the code clean and CI from failing. By installing pre-commit hooks like here, whenever you do a <code>git commit</code> in this repo, a series of checks and cleanup tasks are run, until everything is in a state that will hopefully make Github Actions not complain when you push your changes.</p> <pre><code>pre-commit install\npre-commit install --hook-type commit-msg\n</code></pre> <p>In addition to some Python-specific checks and cleanup tasks, this will also check your commit message so it's in line with the suggested format: https://www.conventionalcommits.org/en/v1.0.0/</p>"},{"location":"development/#run-kiara","title":"Run kiara","text":"<p>To check if everything works as expected and you can start adding/changing code in this repository, run any <code>kiara</code> command:</p> <pre><code>kiara operation list -t language_processing\n</code></pre> <p>If everything is set up correctly, the output of this command should contain a few operations that are implemented in this repository.</p>"},{"location":"usage/","title":"Usage","text":"<p>TO BE DONE</p>"},{"location":"info/SUMMARY/","title":"SUMMARY","text":"<ul> <li>module_types</li> <li>operations</li> </ul>"},{"location":"info/module_types/","title":"module_types","text":""},{"location":"info/module_types/#kiara_info.module_types.generate.LDA.for.tokens_array","title":"<code>generate.LDA.for.tokens_array</code>","text":"<pre>                                                                                \n Documentation                                                                  \n    Perform Latent Dirichlet Allocation on a tokenized    \n                          corpus.                                               \n                          This module computes models for a range of number     \n                          of topics provided by the user.                       \n                       Author(s)                                                                      \n    Markus Binsteiner   markus@frkl.io                    \n Context                                                                        \n  Tags         language_processing, LDA, tokens         \n                        Labels       package:                                 \n                         kiara_plugin.language_processing         \n                        References   source_repo:                             \n                       https://github.com/DHARPA-Project/kia\u2026   \n documentation:                           \n                       https://DHARPA-Project.github.io/kiar\u2026   \n Module config schema                                                           \n  Field       Type     Descript\u2026   Required   Default   \n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \n                          constants   object   Value       no                   \n                                               constants                        \n                                               for this                         \n                                               module.                          \n                          defaults    object   Value       no                   \n                                               defaults                         \n                                               for this                         \n                                               module.                          \n                       Python class                                                                   \n  python_class_name    LDAModule                        \n                        python_module_name   kiara_plugin.language_process\u2026   \n                        full_name            kiara_plugin.language_process\u2026   \n                       Processing source code  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \n                       def process(self, inputs: ValueMap, outputs: Value\u2026   \n from gensim import corpora                        \n     logging.getLogger(\"gensim\").setLevel(logging.E\u2026   \n     tokens_array: KiaraArray = inputs.get_value_da\u2026   \n     tokens = tokens_array.arrow_array.to_pylist()     \n     words_per_topic = inputs.get_value_data(\"words\u2026   \n     num_topics_min = inputs.get_value_data(\"num_to\u2026   \n     num_topics_max = inputs.get_value_data(\"num_to\u2026   \n if not num_topics_max:                            \n         num_topics_max = num_topics_min               \n if num_topics_max &lt; num_topics_min:               \n raise KiaraProcessingException(               \n \"The max number of topics must be larg\u2026   \n         )                                             \n     compute_coherence = inputs.get_value_data(\"com\u2026   \n     id2word = corpora.Dictionary(tokens)              \n     corpus = [id2word.doc2bow(text) for text in to\u2026   \n # model = gensim.models.ldamulticore.LdaMultic\u2026   \n #     corpus, id2word=id2word, num_topics=num_\u2026   \n # )                                               \n     models = {}                                       \n     model_tables = {}                                 \n     coherence = {}                                    \n # multi_threaded = False                          \n # if not multi_threaded:                          \n for nt in range(num_topics_min, num_topics_max\u2026   \n         model = self.create_model(corpus=corpus, n\u2026   \n         models[nt] = model                            \n         topic_print_model = model.print_topics(num\u2026   \n # dbg(topic_print_model)                      \n # df = pd.DataFrame(topic_print_model, col\u2026   \n # TODO: create table directly                 \n # result_table = Table.from_pandas(df)        \n         model_tables[nt] = topic_print_model          \n if compute_coherence:                         \n             coherence_result = self.compute_cohere\u2026   \n                 model=model, corpus_model=tokens, \u2026   \n             )                                         \n             coherence[nt] = coherence_result          \n # else:                                           \n #     def create_model(num_topics):               \n #         model = self.create_model(corpus=cor\u2026   \n #         topic_print_model = model.print_topi\u2026   \n #         df = pd.DataFrame(topic_print_model,\u2026   \n #         # TODO: create table directly           \n #         result_table = Table.from_pandas(df)    \n #         coherence_result = None                 \n #         if compute_coherence:                   \n #             coherence_result = self.compute_\u2026   \n #         return (num_topics, model, result_ta\u2026   \n #                                                 \n #     executor = ThreadPoolExecutor()             \n #     results: typing.Any = executor.map(creat\u2026   \n #     executor.shutdown(wait=True)                \n #     for r in results:                           \n #         models[r[0]] = r[1]                     \n #         model_tables[r[0]] = r[2]               \n #         if compute_coherence:                   \n #             coherence[r[0]] = r[3]              \n # df_coherence = pd.DataFrame(coherence.keys()\u2026   \n # df_coherence[\"Coherence\"] = coherence.values\u2026   \n if compute_coherence:                             \n         coherence_table = self.assemble_coherence(    \n             models_dict=models, words_per_topic=wo\u2026   \n         )                                             \n else:                                             \n         coherence_table = None                        \n     coherence_map = {k: v.item() for k, v in coher\u2026   \n     outputs.set_values(                               \n         topic_models=model_tables,                    \n         coherence_table=coherence_table,              \n         coherence_map=coherence_map,                  \n     )                                                 \n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \n                                                                                \n</pre>"},{"location":"info/module_types/#kiara_info.module_types.tokenize.string","title":"<code>tokenize.string</code>","text":"<pre>                                                                                \n Documentation                                                                  \n    Tokenize a string.                                    \n                       Author(s)                                                                      \n    Markus Binsteiner   markus@frkl.io                    \n Context                                                                        \n  Tags         language_processing                      \n                        Labels       package:                                 \n                         kiara_plugin.language_processing         \n                        References   source_repo:                             \n                       https://github.com/DHARPA-Project/kia\u2026   \n documentation:                           \n                       https://DHARPA-Project.github.io/kiar\u2026   \n Module config schema                                                           \n  Field      Type      Descript\u2026   Required   Default   \n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \n                          constan\u2026   object    Value       no                   \n                                               constants                        \n                                               for this                         \n                                               module.                          \n                          defaults   object    Value       no                   \n                                               defaults                         \n                                               for this                         \n                                               module.                          \n                          filter_\u2026   boolean   Whether     no         true      \n                                               to filter                        \n                                               out non                          \n                                               alpha                            \n                                               tokens.                          \n                          min_tok\u2026   integer   The         no         3         \n                                               minimum                          \n                                               token                            \n                                               length.                          \n                          to_lowe\u2026   boolean   Whether     no         true      \n                                               to                               \n                                               lowercase                        \n                                               the                              \n                                               tokens.                          \n                       Python class                                                                   \n  python_class_name    TokenizeTextModule               \n                        python_module_name   kiara_plugin.language_process\u2026   \n                        full_name            kiara_plugin.language_process\u2026   \n                       Processing source code  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \n                       def process(self, inputs: ValueMap, outputs: Value\u2026   \n import nltk                                       \n # TODO: module-independent caching?               \n # language = inputs.get_value_data(\"language\")    \n #                                                 \n     text = inputs.get_value_data(\"text\")              \n     tokenized = nltk.word_tokenize(text)              \n     result = tokenized                                \n if self.get_config_value(\"min_token_length\") &gt;\u2026   \n         result = (                                    \n             x                                         \n for x in tokenized                        \n if len(x) &gt;= self.get_config_value(\"mi\u2026   \n         )                                             \n if self.get_config_value(\"filter_non_alpha\"):     \n         result = (x for x in result if x.isalpha())   \n if self.get_config_value(\"to_lowercase\"):         \n         result = (x.lower() for x in result)          \n     outputs.set_value(\"token_list\", list(result))     \n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \n                                                                                \n</pre>"},{"location":"info/module_types/#kiara_info.module_types.tokenize.texts_array","title":"<code>tokenize.texts_array</code>","text":"<pre>                                                                                \n Documentation                                                                  \n    Split sentences into words or words into              \n                          characters.                                           \n                          In other words, this operation establishes the word   \n                          boundaries (i.e., tokens) a very helpful way of       \n                          finding patterns. It is also the typical step prior   \n                          to stemming and lemmatization                         \n                       Author(s)                                                                      \n    Markus Binsteiner   markus@frkl.io                    \n Context                                                                        \n  Tags         language_processing, tokenize, tokens    \n                        Labels       package:                                 \n                         kiara_plugin.language_processing         \n                        References   source_repo:                             \n                       https://github.com/DHARPA-Project/kia\u2026   \n documentation:                           \n                       https://DHARPA-Project.github.io/kiar\u2026   \n Module config schema                                                           \n  Field       Type     Descript\u2026   Required   Default   \n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \n                          constants   object   Value       no                   \n                                               constants                        \n                                               for this                         \n                                               module.                          \n                          defaults    object   Value       no                   \n                                               defaults                         \n                                               for this                         \n                                               module.                          \n                       Python class                                                                   \n  python_class_name    TokenizeTextArrayeModule         \n                        python_module_name   kiara_plugin.language_process\u2026   \n                        full_name            kiara_plugin.language_process\u2026   \n                       Processing source code  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \n                       def process(self, inputs: ValueMap, outputs: Value\u2026   \n pass                                              \n import nltk                                       \n import polars as pl                               \n import pyarrow as pa                              \n     array: KiaraArray = inputs.get_value_data(\"tex\u2026   \n # tokenize_by_word: bool = inputs.get_value_da\u2026   \n     column: pa.ChunkedArray = array.arrow_array       \n # warnings.filterwarnings(\"ignore\", category=n\u2026   \n def word_tokenize(word):                          \n         result = nltk.word_tokenize(word)             \n return result                                 \n     series = pl.Series(name=\"tokens\", values=colum\u2026   \n     result = series.apply(word_tokenize)              \n     result_array = result.to_arrow()                  \n # TODO: remove this cast once the array data t\u2026   \n     chunked = pa.chunked_array(result_array)          \n     outputs.set_values(tokens_array=chunked)          \n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \n                                                                                \n</pre>"},{"location":"info/module_types/#kiara_info.module_types.create.stopwords_list","title":"<code>create.stopwords_list</code>","text":"<pre>                                                                                \n Documentation                                                                  \n    Create a list of stopwords from one or multiple       \n                          sources.                                              \n                          This will download nltk stopwords if necessary, and   \n                          merge all input lists into a single, sorted list      \n                          without duplicates.                                   \n                       Author(s)                                                                      \n    Markus Binsteiner   markus@frkl.io                    \n Context                                                                        \n  Tags         language_processing                      \n                        Labels       package:                                 \n                         kiara_plugin.language_processing         \n                        References   source_repo:                             \n                       https://github.com/DHARPA-Project/kia\u2026   \n documentation:                           \n                       https://DHARPA-Project.github.io/kiar\u2026   \n Module config schema                                                           \n  Field       Type     Descript\u2026   Required   Default   \n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \n                          constants   object   Value       no                   \n                                               constants                        \n                                               for this                         \n                                               module.                          \n                          defaults    object   Value       no                   \n                                               defaults                         \n                                               for this                         \n                                               module.                          \n                       Python class                                                                   \n  python_class_name    AssembleStopwordsModule          \n                        python_module_name   kiara_plugin.language_process\u2026   \n                        full_name            kiara_plugin.language_process\u2026   \n                       Processing source code  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \n                       def process(self, inputs: ValueMap, outputs: Value\u2026   \n     stopwords = set()                                 \n     _languages = inputs.get_value_obj(\"languages\")    \n if _languages.is_set:                             \n         all_stopwords = get_stopwords()               \n         languages: ListModel = _languages.data        \n for language in languages.list_data:          \n if language not in all_stopwords.filei\u2026   \n raise KiaraProcessingException(       \n f\"Invalid language: {language}\u2026   \n                 )                                     \n             stopwords.update(get_stopwords().words\u2026   \n     _stopword_lists = inputs.get_value_obj(\"stopwo\u2026   \n if _stopword_lists.is_set:                        \n         stopword_lists: ListModel = _stopword_list\u2026   \n for stopword_list in stopword_lists.list_d\u2026   \n if isinstance(stopword_list, str):        \n                 stopwords.add(stopword_list)          \n else:                                     \n                 stopwords.update(stopword_list)       \n     outputs.set_value(\"stopwords_list\", sorted(sto\u2026   \n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \n                                                                                \n</pre>"},{"location":"info/module_types/#kiara_info.module_types.remove_stopwords.from.tokens_array","title":"<code>remove_stopwords.from.tokens_array</code>","text":"<pre>                                                                                \n Documentation                                                                  \n    Remove stopwords from an array of token-lists.        \n                       Author(s)                                                                      \n    Markus Binsteiner   markus@frkl.io                    \n Context                                                                        \n  Tags         language_processing                      \n                        Labels       package:                                 \n                         kiara_plugin.language_processing         \n                        References   source_repo:                             \n                       https://github.com/DHARPA-Project/kia\u2026   \n documentation:                           \n                       https://DHARPA-Project.github.io/kiar\u2026   \n Module config schema                                                           \n  Field       Type     Descript\u2026   Required   Default   \n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \n                          constants   object   Value       no                   \n                                               constants                        \n                                               for this                         \n                                               module.                          \n                          defaults    object   Value       no                   \n                                               defaults                         \n                                               for this                         \n                                               module.                          \n                       Python class                                                                   \n  python_class_name    RemoveStopwordsModule            \n                        python_module_name   kiara_plugin.language_process\u2026   \n                        full_name            kiara_plugin.language_process\u2026   \n                       Processing source code  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \n                       def process(self, inputs: ValueMap, outputs: Value\u2026   \n import pyarrow as pa                              \n     custom_stopwords = inputs.get_value_data(\"addi\u2026   \n if inputs.get_value_obj(\"languages\").is_set:      \n         _languages: ListModel = inputs.get_value_d\u2026   \n         languages = _languages.list_data              \n else:                                             \n         languages = []                                \n     stopwords = set()                                 \n if languages:                                     \n for language in languages:                    \n if language not in get_stopwords().fil\u2026   \n raise KiaraProcessingException(       \n f\"Invalid language: {language}\u2026   \n                 )                                     \n             stopwords.update(get_stopwords().words\u2026   \n if custom_stopwords:                              \n         stopwords.update(custom_stopwords)            \n     orig_array = inputs.get_value_obj(\"tokens_arra\u2026   \n if not stopwords:                                 \n         outputs.set_value(\"tokens_array\", orig_arr\u2026   \n return                                        \n # if hasattr(orig_array, \"to_pylist\"):            \n #     token_lists = orig_array.to_pylist()        \n     tokens_array = orig_array.data.arrow_array        \n # TODO: use vaex for this                         \n     result = []                                       \n for token_list in tokens_array:                   \n         cleaned_list = [x for x in token_list.as_p\u2026   \n         result.append(cleaned_list)                   \n     outputs.set_value(\"tokens_array\", pa.chunked_a\u2026   \n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \n                                                                                \n</pre>"},{"location":"info/module_types/#kiara_info.module_types.preprocess.tokens_array","title":"<code>preprocess.tokens_array</code>","text":"<pre>                                                                                \n Documentation                                                                  \n    Preprocess lists of tokens, incl. lowercasing,        \n                          remove special characers, etc.                        \n                          Lowercasing: Lowercase the words. This operation is   \n                          a double-edged sword. It can be effective at          \n                          yielding potentially better results in the case of    \n                          relatively small datasets or datatsets with a high    \n                          percentage of OCR mistakes. For instance, if          \n                          lowercasing is not performed, the algorithm will      \n                          treat USA, Usa, usa, UsA, uSA, etc. as distinct       \n                          tokens, even though they may all refer to the same    \n                          entity. On the other hand, if the dataset does not    \n                          contain such OCR mistakes, then it may become         \n                          difficult to distinguish between homonyms and make    \n                          interpreting the topics much harder.                  \n                          Removing stopwords and words with less than three     \n                          characters: Remove low information words. These are   \n                          typically words such as articles, pronouns,           \n                          prepositions, conjunctions, etc. which are not        \n                          semantically salient. There are numerous stopword     \n                          lists available for many, though not all, languages   \n                          which can be easily adapted to the individual         \n                          researcher's needs. Removing words with less than     \n                          three characters may additionally remove many OCR     \n                          mistakes. Both these operations have the dual         \n                          advantage of yielding more reliable results while     \n                          reducing the size of the dataset, thus in turn        \n                          reducing the required processing power. This step     \n                          can therefore hardly be considered optional in TM.    \n                          Noise removal: Remove elements such as punctuation    \n                          marks, special characters, numbers, html              \n                          formatting, etc. This operation is again concerned    \n                          with removing elements that may not be relevant to    \n                          the text analysis and in fact interfere with it.      \n                          Depending on the dataset and research question,       \n                          this operation can become essential.                  \n                       Author(s)                                                                      \n    Markus Binsteiner   markus@frkl.io                    \n Context                                                                        \n  Tags         language_processing, tokens,             \n                         preprocess                               \n                        Labels       package:                                 \n                         kiara_plugin.language_processing         \n                        References   source_repo:                             \n                       https://github.com/DHARPA-Project/kia\u2026   \n documentation:                           \n                       https://DHARPA-Project.github.io/kiar\u2026   \n Module config schema                                                           \n  Field       Type     Descript\u2026   Required   Default   \n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \n                          constants   object   Value       no                   \n                                               constants                        \n                                               for this                         \n                                               module.                          \n                          defaults    object   Value       no                   \n                                               defaults                         \n                                               for this                         \n                                               module.                          \n                       Python class                                                                   \n  python_class_name    PreprocessModule                 \n                        python_module_name   kiara_plugin.language_process\u2026   \n                        full_name            kiara_plugin.language_process\u2026   \n                       Processing source code  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \n                       def process(self, inputs: ValueMap, outputs: Value\u2026   \n import polars as pl                               \n import pyarrow as pa                              \n     tokens_array: KiaraArray = inputs.get_value_da\u2026   \n     lowercase: bool = inputs.get_value_data(\"to_lo\u2026   \n     remove_alphanumeric: bool = inputs.get_value_d\u2026   \n     remove_non_alpha: bool = inputs.get_value_data\u2026   \n     remove_all_numeric: bool = inputs.get_value_da\u2026   \n     remove_short_tokens: int = inputs.get_value_da\u2026   \n if remove_short_tokens is None:                   \n         remove_short_tokens = -1                      \n     _remove_stopwords = inputs.get_value_obj(\"remo\u2026   \n if _remove_stopwords.is_set:                      \n         stopword_list: Union[Iterable[str], None] \u2026   \n else:                                             \n         stopword_list = None                          \n # it's better to have one method every token g\u2026   \n # because that way each token only needs to be\u2026   \n def check_token(token: str) -&gt; Union[str, None\u2026   \n # remove short tokens first, since we can \u2026   \n assert isinstance(remove_short_tokens, int)   \n if remove_short_tokens &gt; 0:                   \n if len(token) &lt;= remove_short_tokens:     \n return None                           \n         _token: str = token                           \n if lowercase:                                 \n             _token = _token.lower()                   \n if remove_non_alpha:                          \n             match = _token if _token.isalpha() els\u2026   \n if match is None:                         \n return None                           \n # if remove_non_alpha was set, we don't ne\u2026   \n if remove_alphanumeric and not remove_non_\u2026   \n             match = _token if _token.isalnum() els\u2026   \n if match is None:                         \n return None                           \n # all-number tokens are already filtered o\u2026   \n if remove_all_numeric and not remove_non_a\u2026   \n             match = None if _token.isdigit() else \u2026   \n if match is None:                         \n return None                           \n if stopword_list and _token and _token.low\u2026   \n return None                               \n return _token                                 \n     series = pl.Series(name=\"tokens\", values=token\u2026   \n     result = series.apply(                            \n lambda token_list: [                          \n             x for x in (check_token(token) for tok\u2026   \n         ]                                             \n     )                                                 \n     result_array = result.to_arrow()                  \n # TODO: remove this cast once the array data t\u2026   \n     chunked = pa.chunked_array(result_array)          \n     outputs.set_values(tokens_array=chunked)          \n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \n                                                                                \n</pre>"},{"location":"info/operations/","title":"operations","text":""},{"location":"info/operations/#kiara_info.operations.create.stopwords_list","title":"<code>create.stopwords_list</code>","text":"Documentation <p>Create a list of stopwords from one or multiple sources.</p> <p>This will download nltk stopwords if necessary, and merge all input lists into a single, sorted list without duplicates.</p> Inputs field name type description required default languages list A list of languages, will be used to retrieve language-specific stopword from nltk. no stopword_list list A list of additional, custom stopwords. no Outputs field name type description required default stopwords_list list A sorted list of unique stopwords. yes"},{"location":"info/operations/#kiara_info.operations.generate.LDA.for.tokens_array","title":"<code>generate.LDA.for.tokens_array</code>","text":"Documentation <p>Perform Latent Dirichlet Allocation on a tokenized corpus.</p> <p>This module computes models for a range of number of topics provided by the user.</p> Inputs field name type description required default tokens_array array The text corpus. yes num_topics_min integer The minimal number of topics. no 7 num_topics_max integer The max number of topics. no 7 compute_coherence boolean Whether to compute the coherence score for each model. no False words_per_topic integer How many words per topic to put in the result model. no 10 Outputs field name type description required default topic_models dict A dictionary with one coherence model table for each number of topics. yes coherence_table table Coherence details. no coherence_map dict A map with the coherence value for every number of topics. yes"},{"location":"info/operations/#kiara_info.operations.preprocess.tokens_array","title":"<code>preprocess.tokens_array</code>","text":"Documentation <p>Preprocess lists of tokens, incl. lowercasing, remove special characers, etc.</p> <p>Lowercasing: Lowercase the words. This operation is a double-edged sword. It can be effective at yielding potentially better results in the case of relatively small datasets or datatsets with a high percentage of OCR mistakes. For instance, if lowercasing is not performed, the algorithm will treat USA, Usa, usa, UsA, uSA, etc. as distinct tokens, even though they may all refer to the same entity. On the other hand, if the dataset does not contain such OCR mistakes, then it may become difficult to distinguish between homonyms and make interpreting the topics much harder.</p> <p>Removing stopwords and words with less than three characters: Remove low information words. These are typically words such as articles, pronouns, prepositions, conjunctions, etc. which are not semantically salient. There are numerous stopword lists available for many, though not all, languages which can be easily adapted to the individual researcher's needs. Removing words with less than three characters may additionally remove many OCR mistakes. Both these operations have the dual advantage of yielding more reliable results while reducing the size of the dataset, thus in turn reducing the required processing power. This step can therefore hardly be considered optional in TM.</p> <p>Noise removal: Remove elements such as punctuation marks, special characters, numbers, html formatting, etc. This operation is again concerned with removing elements that may not be relevant to the text analysis and in fact interfere with it. Depending on the dataset and research question, this operation can become essential.</p> Inputs field name type description required default tokens_array array The tokens array to pre-process. yes to_lowercase boolean Apply lowercasing to the text. no False remove_alphanumeric boolean Remove all tokens that include numbers (e.g. ex1ample). no False remove_non_alpha boolean Remove all tokens that include punctuation and numbers (e.g. ex1a.mple). no False remove_all_numeric boolean Remove all tokens that contain numbers only (e.g. 876). no False remove_short_tokens integer Remove tokens shorter or equal to this value. If value is &lt;= 0, no filtering will be done. no 0 remove_stopwords list Remove stopwords. no Outputs field name type description required default tokens_array array The pre-processed content, as an array of lists of strings. yes"},{"location":"info/operations/#kiara_info.operations.remove_stopwords.from.tokens_array","title":"<code>remove_stopwords.from.tokens_array</code>","text":"Documentation <p>Remove stopwords from an array of token-lists.</p> Inputs field name type description required default tokens_array array An array of string lists (a list of tokens). yes languages list A list of language names to use default stopword lists for. no additional_stopwords list A list of additional, custom stopwords. no Outputs field name type description required default tokens_array array An array of string lists, with the stopwords removed. yes"},{"location":"info/operations/#kiara_info.operations.tokenize.string","title":"<code>tokenize.string</code>","text":"Documentation <p>Tokenize a string.</p> Inputs field name type description required default text string The text to tokenize. yes Outputs field name type description required default token_list list The tokenized version of the input text. yes"},{"location":"info/operations/#kiara_info.operations.tokenize.texts_array","title":"<code>tokenize.texts_array</code>","text":"Documentation <p>Split sentences into words or words into characters.</p> <p>In other words, this operation establishes the word boundaries (i.e., tokens) a very helpful way of finding patterns. It is also the typical step prior to stemming and lemmatization</p> Inputs field name type description required default texts_array array An array of text items to be tokenized. yes tokenize_by_word boolean Whether to tokenize by word (default), or character. no True Outputs field name type description required default tokens_array array The tokenized content, as an array of lists of strings. yes"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>kiara_plugin<ul> <li>language_processing<ul> <li>data_types</li> <li>models</li> <li>modules<ul> <li>lda</li> <li>lemmatize</li> <li>tokens</li> </ul> </li> <li>pipelines</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/kiara_plugin/language_processing/__init__/","title":"language_processing","text":"<p>Top-level package for kiara_plugin.language_processing.</p>"},{"location":"reference/kiara_plugin/language_processing/__init__/#kiara_plugin.language_processing-attributes","title":"Attributes","text":""},{"location":"reference/kiara_plugin/language_processing/__init__/#kiara_plugin.language_processing.KIARA_METADATA","title":"<code>KIARA_METADATA = {'authors': [{'name': __author__, 'email': __email__}], 'description': 'Kiara modules for: language_processing', 'references': {'source_repo': {'desc': 'The module package git repository.', 'url': 'https://github.com/DHARPA-Project/kiara_plugin.language_processing'}, 'documentation': {'desc': 'The url for the module package documentation.', 'url': 'https://DHARPA-Project.github.io/kiara_plugin.language_processing/'}}, 'tags': ['language_processing'], 'labels': {'package': 'kiara_plugin.language_processing'}}</code>  <code>module-attribute</code>","text":""},{"location":"reference/kiara_plugin/language_processing/__init__/#kiara_plugin.language_processing.find_modules","title":"<code>find_modules: KiaraEntryPointItem = (find_kiara_modules_under, 'kiara_plugin.language_processing.modules')</code>  <code>module-attribute</code>","text":""},{"location":"reference/kiara_plugin/language_processing/__init__/#kiara_plugin.language_processing.find_model_classes","title":"<code>find_model_classes: KiaraEntryPointItem = (find_kiara_model_classes_under, 'kiara_plugin.language_processing.models')</code>  <code>module-attribute</code>","text":""},{"location":"reference/kiara_plugin/language_processing/__init__/#kiara_plugin.language_processing.find_data_types","title":"<code>find_data_types: KiaraEntryPointItem = (find_data_types_under, 'kiara_plugin.language_processing.data_types')</code>  <code>module-attribute</code>","text":""},{"location":"reference/kiara_plugin/language_processing/__init__/#kiara_plugin.language_processing.find_pipelines","title":"<code>find_pipelines: KiaraEntryPointItem = (find_pipeline_base_path_for_module, 'kiara_plugin.language_processing.pipelines', KIARA_METADATA)</code>  <code>module-attribute</code>","text":""},{"location":"reference/kiara_plugin/language_processing/__init__/#kiara_plugin.language_processing-functions","title":"Functions","text":""},{"location":"reference/kiara_plugin/language_processing/__init__/#kiara_plugin.language_processing.get_version","title":"<code>get_version()</code>","text":"Source code in <code>kiara_plugin/language_processing/__init__.py</code> <pre><code>def get_version():\n    from pkg_resources import DistributionNotFound, get_distribution\n\n    try:\n        # Change here if project is renamed and does not equal the package name\n        dist_name = __name__\n        __version__ = get_distribution(dist_name).version\n    except DistributionNotFound:\n\n        try:\n            version_file = os.path.join(os.path.dirname(__file__), \"version.txt\")\n\n            if os.path.exists(version_file):\n                with open(version_file, encoding=\"utf-8\") as vf:\n                    __version__ = vf.read()\n            else:\n                __version__ = \"unknown\"\n\n        except (Exception):\n            pass\n\n        if __version__ is None:\n            __version__ = \"unknown\"\n\n    return __version__\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/data_types/","title":"data_types","text":"<p>This module contains the value type classes that are used in the <code>kiara_plugin.language_processing</code> package.</p>"},{"location":"reference/kiara_plugin/language_processing/models/","title":"models","text":"<p>This module contains the metadata (and other) models that are used in the <code>kiara_plugin.language_processing</code> package.</p> <p>Those models are convenience wrappers that make it easier for kiara to find, create, manage and version metadata -- but also other type of models -- that is attached to data, as well as kiara modules.</p> <p>Metadata models must be a sub-class of kiara.metadata.MetadataModel. Other models usually sub-class a pydantic BaseModel or implement custom base classes.</p>"},{"location":"reference/kiara_plugin/language_processing/modules/__init__/","title":"modules","text":""},{"location":"reference/kiara_plugin/language_processing/modules/lda/","title":"lda","text":""},{"location":"reference/kiara_plugin/language_processing/modules/lda/#kiara_plugin.language_processing.modules.lda-classes","title":"Classes","text":""},{"location":"reference/kiara_plugin/language_processing/modules/lda/#kiara_plugin.language_processing.modules.lda.LDAModule","title":"<code>LDAModule</code>","text":"<p>         Bases: <code>KiaraModule</code></p> <p>Perform Latent Dirichlet Allocation on a tokenized corpus.</p> <p>This module computes models for a range of number of topics provided by the user.</p> Source code in <code>kiara_plugin/language_processing/modules/lda.py</code> <pre><code>class LDAModule(KiaraModule):\n\"\"\"Perform Latent Dirichlet Allocation on a tokenized corpus.\n\n    This module computes models for a range of number of topics provided by the user.\n    \"\"\"\n\n    _module_type_name = \"generate.LDA.for.tokens_array\"\n\n    KIARA_METADATA = {\n        \"tags\": [\"LDA\", \"tokens\"],\n    }\n\n    def create_inputs_schema(\n        self,\n    ) -&gt; ValueSetSchema:\n\n        inputs: Dict[str, Dict[str, Any]] = {\n            \"tokens_array\": {\"type\": \"array\", \"doc\": \"The text corpus.\"},\n            \"num_topics_min\": {\n                \"type\": \"integer\",\n                \"doc\": \"The minimal number of topics.\",\n                \"default\": 7,\n            },\n            \"num_topics_max\": {\n                \"type\": \"integer\",\n                \"doc\": \"The max number of topics.\",\n                \"default\": 7,\n                \"optional\": True,\n            },\n            \"compute_coherence\": {\n                \"type\": \"boolean\",\n                \"doc\": \"Whether to compute the coherence score for each model.\",\n                \"default\": False,\n            },\n            \"words_per_topic\": {\n                \"type\": \"integer\",\n                \"doc\": \"How many words per topic to put in the result model.\",\n                \"default\": 10,\n            },\n        }\n        return inputs\n\n    def create_outputs_schema(\n        self,\n    ) -&gt; ValueSetSchema:\n\n        outputs: Mapping[str, Mapping[str, Any]] = {\n            \"topic_models\": {\n                \"type\": \"dict\",\n                \"doc\": \"A dictionary with one coherence model table for each number of topics.\",\n            },\n            \"coherence_table\": {\n                \"type\": \"table\",\n                \"doc\": \"Coherence details.\",\n                \"optional\": True,\n            },\n            \"coherence_map\": {\n                \"type\": \"dict\",\n                \"doc\": \"A map with the coherence value for every number of topics.\",\n            },\n        }\n        return outputs\n\n    def create_model(self, corpus, num_topics: int, id2word: Mapping[str, int]):\n        from gensim.models import LdaModel\n\n        model = LdaModel(\n            corpus, id2word=id2word, num_topics=num_topics, eval_every=None\n        )\n        return model\n\n    def compute_coherence(self, model, corpus_model, id2word: Mapping[str, int]):\n\n        from gensim.models import CoherenceModel\n\n        coherencemodel = CoherenceModel(\n            model=model,\n            texts=corpus_model,\n            dictionary=id2word,\n            coherence=\"c_v\",\n            processes=1,\n        )\n        coherence_value = coherencemodel.get_coherence()\n        return coherence_value\n\n    def assemble_coherence(self, models_dict: Mapping[int, Any], words_per_topic: int):\n\n        import pandas as pd\n        import pyarrow as pa\n\n        # Create list with topics and topic words for each number of topics\n        num_topics_list = []\n        topics_list = []\n        for (\n            num_topics,\n            model,\n        ) in models_dict.items():\n\n            num_topics_list.append(num_topics)\n            topic_print = model.print_topics(num_words=words_per_topic)\n            topics_list.append(topic_print)\n\n        df_coherence_table = pd.DataFrame(columns=[\"topic_id\", \"words\", \"num_topics\"])\n\n        idx = 0\n        for i in range(len(topics_list)):\n            for j in range(len(topics_list[i])):\n                df_coherence_table.loc[idx] = \"\"\n                df_coherence_table[\"topic_id\"].loc[idx] = j + 1\n                df_coherence_table[\"words\"].loc[idx] = \", \".join(\n                    re.findall(r'\"(\\w+)\"', topics_list[i][j][1])\n                )\n                df_coherence_table[\"num_topics\"].loc[idx] = num_topics_list[i]\n                idx += 1\n\n        coherence_table = pa.Table.from_pandas(df_coherence_table, preserve_index=False)\n        return coherence_table\n\n    def process(self, inputs: ValueMap, outputs: ValueMap) -&gt; None:\n\n        from gensim import corpora\n\n        logging.getLogger(\"gensim\").setLevel(logging.ERROR)\n        tokens_array: KiaraArray = inputs.get_value_data(\"tokens_array\")\n        tokens = tokens_array.arrow_array.to_pylist()\n\n        words_per_topic = inputs.get_value_data(\"words_per_topic\")\n\n        num_topics_min = inputs.get_value_data(\"num_topics_min\")\n        num_topics_max = inputs.get_value_data(\"num_topics_max\")\n        if not num_topics_max:\n            num_topics_max = num_topics_min\n\n        if num_topics_max &lt; num_topics_min:\n            raise KiaraProcessingException(\n                \"The max number of topics must be larger or equal to the min number of topics.\"\n            )\n\n        compute_coherence = inputs.get_value_data(\"compute_coherence\")\n        id2word = corpora.Dictionary(tokens)\n        corpus = [id2word.doc2bow(text) for text in tokens]\n\n        # model = gensim.models.ldamulticore.LdaMulticore(\n        #     corpus, id2word=id2word, num_topics=num_topics, eval_every=None\n        # )\n\n        models = {}\n        model_tables = {}\n        coherence = {}\n\n        # multi_threaded = False\n        # if not multi_threaded:\n\n        for nt in range(num_topics_min, num_topics_max + 1):\n            model = self.create_model(corpus=corpus, num_topics=nt, id2word=id2word)\n            models[nt] = model\n            topic_print_model = model.print_topics(num_words=words_per_topic)\n            # dbg(topic_print_model)\n            # df = pd.DataFrame(topic_print_model, columns=[\"topic_id\", \"words\"])\n            # TODO: create table directly\n            # result_table = Table.from_pandas(df)\n            model_tables[nt] = topic_print_model\n\n            if compute_coherence:\n                coherence_result = self.compute_coherence(\n                    model=model, corpus_model=tokens, id2word=id2word\n                )\n                coherence[nt] = coherence_result\n\n        # else:\n        #     def create_model(num_topics):\n        #         model = self.create_model(corpus=corpus, num_topics=num_topics, id2word=id2word)\n        #         topic_print_model = model.print_topics(num_words=30)\n        #         df = pd.DataFrame(topic_print_model, columns=[\"topic_id\", \"words\"])\n        #         # TODO: create table directly\n        #         result_table = Table.from_pandas(df)\n        #         coherence_result = None\n        #         if compute_coherence:\n        #             coherence_result = self.compute_coherence(model=model, corpus_model=tokens, id2word=id2word)\n        #         return (num_topics, model, result_table, coherence_result)\n        #\n        #     executor = ThreadPoolExecutor()\n        #     results: typing.Any = executor.map(create_model, range(num_topics_min, num_topics_max+1))\n        #     executor.shutdown(wait=True)\n        #     for r in results:\n        #         models[r[0]] = r[1]\n        #         model_tables[r[0]] = r[2]\n        #         if compute_coherence:\n        #             coherence[r[0]] = r[3]\n\n        # df_coherence = pd.DataFrame(coherence.keys(), columns=[\"Number of topics\"])\n        # df_coherence[\"Coherence\"] = coherence.values()\n\n        if compute_coherence:\n            coherence_table = self.assemble_coherence(\n                models_dict=models, words_per_topic=words_per_topic\n            )\n        else:\n            coherence_table = None\n\n        coherence_map = {k: v.item() for k, v in coherence.items()}\n\n        outputs.set_values(\n            topic_models=model_tables,\n            coherence_table=coherence_table,\n            coherence_map=coherence_map,\n        )\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/lda/#kiara_plugin.language_processing.modules.lda.LDAModule-attributes","title":"Attributes","text":""},{"location":"reference/kiara_plugin/language_processing/modules/lda/#kiara_plugin.language_processing.modules.lda.LDAModule.KIARA_METADATA","title":"<code>KIARA_METADATA = {'tags': ['LDA', 'tokens']}</code>  <code>class-attribute</code>","text":""},{"location":"reference/kiara_plugin/language_processing/modules/lda/#kiara_plugin.language_processing.modules.lda.LDAModule-functions","title":"Functions","text":""},{"location":"reference/kiara_plugin/language_processing/modules/lda/#kiara_plugin.language_processing.modules.lda.LDAModule.create_inputs_schema","title":"<code>create_inputs_schema() -&gt; ValueSetSchema</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/lda.py</code> <pre><code>def create_inputs_schema(\n    self,\n) -&gt; ValueSetSchema:\n\n    inputs: Dict[str, Dict[str, Any]] = {\n        \"tokens_array\": {\"type\": \"array\", \"doc\": \"The text corpus.\"},\n        \"num_topics_min\": {\n            \"type\": \"integer\",\n            \"doc\": \"The minimal number of topics.\",\n            \"default\": 7,\n        },\n        \"num_topics_max\": {\n            \"type\": \"integer\",\n            \"doc\": \"The max number of topics.\",\n            \"default\": 7,\n            \"optional\": True,\n        },\n        \"compute_coherence\": {\n            \"type\": \"boolean\",\n            \"doc\": \"Whether to compute the coherence score for each model.\",\n            \"default\": False,\n        },\n        \"words_per_topic\": {\n            \"type\": \"integer\",\n            \"doc\": \"How many words per topic to put in the result model.\",\n            \"default\": 10,\n        },\n    }\n    return inputs\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/lda/#kiara_plugin.language_processing.modules.lda.LDAModule.create_outputs_schema","title":"<code>create_outputs_schema() -&gt; ValueSetSchema</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/lda.py</code> <pre><code>def create_outputs_schema(\n    self,\n) -&gt; ValueSetSchema:\n\n    outputs: Mapping[str, Mapping[str, Any]] = {\n        \"topic_models\": {\n            \"type\": \"dict\",\n            \"doc\": \"A dictionary with one coherence model table for each number of topics.\",\n        },\n        \"coherence_table\": {\n            \"type\": \"table\",\n            \"doc\": \"Coherence details.\",\n            \"optional\": True,\n        },\n        \"coherence_map\": {\n            \"type\": \"dict\",\n            \"doc\": \"A map with the coherence value for every number of topics.\",\n        },\n    }\n    return outputs\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/lda/#kiara_plugin.language_processing.modules.lda.LDAModule.create_model","title":"<code>create_model(corpus, num_topics: int, id2word: Mapping[str, int])</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/lda.py</code> <pre><code>def create_model(self, corpus, num_topics: int, id2word: Mapping[str, int]):\n    from gensim.models import LdaModel\n\n    model = LdaModel(\n        corpus, id2word=id2word, num_topics=num_topics, eval_every=None\n    )\n    return model\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/lda/#kiara_plugin.language_processing.modules.lda.LDAModule.compute_coherence","title":"<code>compute_coherence(model, corpus_model, id2word: Mapping[str, int])</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/lda.py</code> <pre><code>def compute_coherence(self, model, corpus_model, id2word: Mapping[str, int]):\n\n    from gensim.models import CoherenceModel\n\n    coherencemodel = CoherenceModel(\n        model=model,\n        texts=corpus_model,\n        dictionary=id2word,\n        coherence=\"c_v\",\n        processes=1,\n    )\n    coherence_value = coherencemodel.get_coherence()\n    return coherence_value\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/lda/#kiara_plugin.language_processing.modules.lda.LDAModule.assemble_coherence","title":"<code>assemble_coherence(models_dict: Mapping[int, Any], words_per_topic: int)</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/lda.py</code> <pre><code>def assemble_coherence(self, models_dict: Mapping[int, Any], words_per_topic: int):\n\n    import pandas as pd\n    import pyarrow as pa\n\n    # Create list with topics and topic words for each number of topics\n    num_topics_list = []\n    topics_list = []\n    for (\n        num_topics,\n        model,\n    ) in models_dict.items():\n\n        num_topics_list.append(num_topics)\n        topic_print = model.print_topics(num_words=words_per_topic)\n        topics_list.append(topic_print)\n\n    df_coherence_table = pd.DataFrame(columns=[\"topic_id\", \"words\", \"num_topics\"])\n\n    idx = 0\n    for i in range(len(topics_list)):\n        for j in range(len(topics_list[i])):\n            df_coherence_table.loc[idx] = \"\"\n            df_coherence_table[\"topic_id\"].loc[idx] = j + 1\n            df_coherence_table[\"words\"].loc[idx] = \", \".join(\n                re.findall(r'\"(\\w+)\"', topics_list[i][j][1])\n            )\n            df_coherence_table[\"num_topics\"].loc[idx] = num_topics_list[i]\n            idx += 1\n\n    coherence_table = pa.Table.from_pandas(df_coherence_table, preserve_index=False)\n    return coherence_table\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/lda/#kiara_plugin.language_processing.modules.lda.LDAModule.process","title":"<code>process(inputs: ValueMap, outputs: ValueMap) -&gt; None</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/lda.py</code> <pre><code>def process(self, inputs: ValueMap, outputs: ValueMap) -&gt; None:\n\n    from gensim import corpora\n\n    logging.getLogger(\"gensim\").setLevel(logging.ERROR)\n    tokens_array: KiaraArray = inputs.get_value_data(\"tokens_array\")\n    tokens = tokens_array.arrow_array.to_pylist()\n\n    words_per_topic = inputs.get_value_data(\"words_per_topic\")\n\n    num_topics_min = inputs.get_value_data(\"num_topics_min\")\n    num_topics_max = inputs.get_value_data(\"num_topics_max\")\n    if not num_topics_max:\n        num_topics_max = num_topics_min\n\n    if num_topics_max &lt; num_topics_min:\n        raise KiaraProcessingException(\n            \"The max number of topics must be larger or equal to the min number of topics.\"\n        )\n\n    compute_coherence = inputs.get_value_data(\"compute_coherence\")\n    id2word = corpora.Dictionary(tokens)\n    corpus = [id2word.doc2bow(text) for text in tokens]\n\n    # model = gensim.models.ldamulticore.LdaMulticore(\n    #     corpus, id2word=id2word, num_topics=num_topics, eval_every=None\n    # )\n\n    models = {}\n    model_tables = {}\n    coherence = {}\n\n    # multi_threaded = False\n    # if not multi_threaded:\n\n    for nt in range(num_topics_min, num_topics_max + 1):\n        model = self.create_model(corpus=corpus, num_topics=nt, id2word=id2word)\n        models[nt] = model\n        topic_print_model = model.print_topics(num_words=words_per_topic)\n        # dbg(topic_print_model)\n        # df = pd.DataFrame(topic_print_model, columns=[\"topic_id\", \"words\"])\n        # TODO: create table directly\n        # result_table = Table.from_pandas(df)\n        model_tables[nt] = topic_print_model\n\n        if compute_coherence:\n            coherence_result = self.compute_coherence(\n                model=model, corpus_model=tokens, id2word=id2word\n            )\n            coherence[nt] = coherence_result\n\n    # else:\n    #     def create_model(num_topics):\n    #         model = self.create_model(corpus=corpus, num_topics=num_topics, id2word=id2word)\n    #         topic_print_model = model.print_topics(num_words=30)\n    #         df = pd.DataFrame(topic_print_model, columns=[\"topic_id\", \"words\"])\n    #         # TODO: create table directly\n    #         result_table = Table.from_pandas(df)\n    #         coherence_result = None\n    #         if compute_coherence:\n    #             coherence_result = self.compute_coherence(model=model, corpus_model=tokens, id2word=id2word)\n    #         return (num_topics, model, result_table, coherence_result)\n    #\n    #     executor = ThreadPoolExecutor()\n    #     results: typing.Any = executor.map(create_model, range(num_topics_min, num_topics_max+1))\n    #     executor.shutdown(wait=True)\n    #     for r in results:\n    #         models[r[0]] = r[1]\n    #         model_tables[r[0]] = r[2]\n    #         if compute_coherence:\n    #             coherence[r[0]] = r[3]\n\n    # df_coherence = pd.DataFrame(coherence.keys(), columns=[\"Number of topics\"])\n    # df_coherence[\"Coherence\"] = coherence.values()\n\n    if compute_coherence:\n        coherence_table = self.assemble_coherence(\n            models_dict=models, words_per_topic=words_per_topic\n        )\n    else:\n        coherence_table = None\n\n    coherence_map = {k: v.item() for k, v in coherence.items()}\n\n    outputs.set_values(\n        topic_models=model_tables,\n        coherence_table=coherence_table,\n        coherence_map=coherence_map,\n    )\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/lemmatize/","title":"lemmatize","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/","title":"tokens","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens-attributes","title":"Attributes","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.log","title":"<code>log = structlog.getLogger()</code>  <code>module-attribute</code>","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens-classes","title":"Classes","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.TokenizeTextConfig","title":"<code>TokenizeTextConfig</code>","text":"<p>         Bases: <code>KiaraModuleConfig</code></p> Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>class TokenizeTextConfig(KiaraModuleConfig):\n\n    filter_non_alpha: bool = Field(\n        description=\"Whether to filter out non alpha tokens.\", default=True\n    )\n    min_token_length: int = Field(description=\"The minimum token length.\", default=3)\n    to_lowercase: bool = Field(\n        description=\"Whether to lowercase the tokens.\", default=True\n    )\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.TokenizeTextConfig-attributes","title":"Attributes","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.TokenizeTextConfig.filter_non_alpha","title":"<code>filter_non_alpha: bool = Field(description='Whether to filter out non alpha tokens.', default=True)</code>  <code>class-attribute</code>","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.TokenizeTextConfig.min_token_length","title":"<code>min_token_length: int = Field(description='The minimum token length.', default=3)</code>  <code>class-attribute</code>","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.TokenizeTextConfig.to_lowercase","title":"<code>to_lowercase: bool = Field(description='Whether to lowercase the tokens.', default=True)</code>  <code>class-attribute</code>","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.TokenizeTextModule","title":"<code>TokenizeTextModule</code>","text":"<p>         Bases: <code>KiaraModule</code></p> <p>Tokenize a string.</p> Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>class TokenizeTextModule(KiaraModule):\n\"\"\"Tokenize a string.\"\"\"\n\n    _config_cls = TokenizeTextConfig\n    _module_type_name = \"tokenize.string\"\n\n    def create_inputs_schema(\n        self,\n    ) -&gt; ValueSetSchema:\n\n        inputs = {\"text\": {\"type\": \"string\", \"doc\": \"The text to tokenize.\"}}\n\n        return inputs\n\n    def create_outputs_schema(\n        self,\n    ) -&gt; ValueSetSchema:\n\n        outputs = {\n            \"token_list\": {\n                \"type\": \"list\",\n                \"doc\": \"The tokenized version of the input text.\",\n            }\n        }\n        return outputs\n\n    def process(self, inputs: ValueMap, outputs: ValueMap) -&gt; None:\n\n        import nltk\n\n        # TODO: module-independent caching?\n        # language = inputs.get_value_data(\"language\")\n        #\n        text = inputs.get_value_data(\"text\")\n        tokenized = nltk.word_tokenize(text)\n\n        result = tokenized\n        if self.get_config_value(\"min_token_length\") &gt; 0:\n            result = (\n                x\n                for x in tokenized\n                if len(x) &gt;= self.get_config_value(\"min_token_length\")\n            )\n\n        if self.get_config_value(\"filter_non_alpha\"):\n            result = (x for x in result if x.isalpha())\n\n        if self.get_config_value(\"to_lowercase\"):\n            result = (x.lower() for x in result)\n\n        outputs.set_value(\"token_list\", list(result))\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.TokenizeTextModule-attributes","title":"Attributes","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.TokenizeTextModule._config_cls","title":"<code>_config_cls = TokenizeTextConfig</code>  <code>class-attribute</code>","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.TokenizeTextModule-functions","title":"Functions","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.TokenizeTextModule.create_inputs_schema","title":"<code>create_inputs_schema() -&gt; ValueSetSchema</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>def create_inputs_schema(\n    self,\n) -&gt; ValueSetSchema:\n\n    inputs = {\"text\": {\"type\": \"string\", \"doc\": \"The text to tokenize.\"}}\n\n    return inputs\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.TokenizeTextModule.create_outputs_schema","title":"<code>create_outputs_schema() -&gt; ValueSetSchema</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>def create_outputs_schema(\n    self,\n) -&gt; ValueSetSchema:\n\n    outputs = {\n        \"token_list\": {\n            \"type\": \"list\",\n            \"doc\": \"The tokenized version of the input text.\",\n        }\n    }\n    return outputs\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.TokenizeTextModule.process","title":"<code>process(inputs: ValueMap, outputs: ValueMap) -&gt; None</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>def process(self, inputs: ValueMap, outputs: ValueMap) -&gt; None:\n\n    import nltk\n\n    # TODO: module-independent caching?\n    # language = inputs.get_value_data(\"language\")\n    #\n    text = inputs.get_value_data(\"text\")\n    tokenized = nltk.word_tokenize(text)\n\n    result = tokenized\n    if self.get_config_value(\"min_token_length\") &gt; 0:\n        result = (\n            x\n            for x in tokenized\n            if len(x) &gt;= self.get_config_value(\"min_token_length\")\n        )\n\n    if self.get_config_value(\"filter_non_alpha\"):\n        result = (x for x in result if x.isalpha())\n\n    if self.get_config_value(\"to_lowercase\"):\n        result = (x.lower() for x in result)\n\n    outputs.set_value(\"token_list\", list(result))\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.TokenizeTextArrayeModule","title":"<code>TokenizeTextArrayeModule</code>","text":"<p>         Bases: <code>KiaraModule</code></p> <p>Split sentences into words or words into characters. In other words, this operation establishes the word boundaries (i.e., tokens) a very helpful way of finding patterns. It is also the typical step prior to stemming and lemmatization</p> Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>class TokenizeTextArrayeModule(KiaraModule):\n\"\"\"Split sentences into words or words into characters.\n    In other words, this operation establishes the word boundaries (i.e., tokens) a very helpful way of finding patterns. It is also the typical step prior to stemming and lemmatization\n    \"\"\"\n\n    _module_type_name = \"tokenize.texts_array\"\n\n    KIARA_METADATA = {\n        \"tags\": [\"tokenize\", \"tokens\"],\n    }\n\n    def create_inputs_schema(\n        self,\n    ) -&gt; ValueSetSchema:\n\n        return {\n            \"texts_array\": {\n                \"type\": \"array\",\n                \"doc\": \"An array of text items to be tokenized.\",\n            },\n            \"tokenize_by_word\": {\n                \"type\": \"boolean\",\n                \"doc\": \"Whether to tokenize by word (default), or character.\",\n                \"default\": True,\n            },\n        }\n\n    def create_outputs_schema(\n        self,\n    ) -&gt; ValueSetSchema:\n\n        return {\n            \"tokens_array\": {\n                \"type\": \"array\",\n                \"doc\": \"The tokenized content, as an array of lists of strings.\",\n            }\n        }\n\n    def process(self, inputs: ValueMap, outputs: ValueMap):\n\n        pass\n\n        import nltk\n        import polars as pl\n        import pyarrow as pa\n\n        array: KiaraArray = inputs.get_value_data(\"texts_array\")\n        # tokenize_by_word: bool = inputs.get_value_data(\"tokenize_by_word\")\n\n        column: pa.ChunkedArray = array.arrow_array\n\n        # warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n\n        def word_tokenize(word):\n            result = nltk.word_tokenize(word)\n            return result\n\n        series = pl.Series(name=\"tokens\", values=column)\n        result = series.apply(word_tokenize)\n\n        result_array = result.to_arrow()\n\n        # TODO: remove this cast once the array data type can handle non-chunked arrays\n        chunked = pa.chunked_array(result_array)\n        outputs.set_values(tokens_array=chunked)\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.TokenizeTextArrayeModule-attributes","title":"Attributes","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.TokenizeTextArrayeModule.KIARA_METADATA","title":"<code>KIARA_METADATA = {'tags': ['tokenize', 'tokens']}</code>  <code>class-attribute</code>","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.TokenizeTextArrayeModule-functions","title":"Functions","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.TokenizeTextArrayeModule.create_inputs_schema","title":"<code>create_inputs_schema() -&gt; ValueSetSchema</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>def create_inputs_schema(\n    self,\n) -&gt; ValueSetSchema:\n\n    return {\n        \"texts_array\": {\n            \"type\": \"array\",\n            \"doc\": \"An array of text items to be tokenized.\",\n        },\n        \"tokenize_by_word\": {\n            \"type\": \"boolean\",\n            \"doc\": \"Whether to tokenize by word (default), or character.\",\n            \"default\": True,\n        },\n    }\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.TokenizeTextArrayeModule.create_outputs_schema","title":"<code>create_outputs_schema() -&gt; ValueSetSchema</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>def create_outputs_schema(\n    self,\n) -&gt; ValueSetSchema:\n\n    return {\n        \"tokens_array\": {\n            \"type\": \"array\",\n            \"doc\": \"The tokenized content, as an array of lists of strings.\",\n        }\n    }\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.TokenizeTextArrayeModule.process","title":"<code>process(inputs: ValueMap, outputs: ValueMap)</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>def process(self, inputs: ValueMap, outputs: ValueMap):\n\n    pass\n\n    import nltk\n    import polars as pl\n    import pyarrow as pa\n\n    array: KiaraArray = inputs.get_value_data(\"texts_array\")\n    # tokenize_by_word: bool = inputs.get_value_data(\"tokenize_by_word\")\n\n    column: pa.ChunkedArray = array.arrow_array\n\n    # warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n\n    def word_tokenize(word):\n        result = nltk.word_tokenize(word)\n        return result\n\n    series = pl.Series(name=\"tokens\", values=column)\n    result = series.apply(word_tokenize)\n\n    result_array = result.to_arrow()\n\n    # TODO: remove this cast once the array data type can handle non-chunked arrays\n    chunked = pa.chunked_array(result_array)\n    outputs.set_values(tokens_array=chunked)\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.AssembleStopwordsModule","title":"<code>AssembleStopwordsModule</code>","text":"<p>         Bases: <code>KiaraModule</code></p> <p>Create a list of stopwords from one or multiple sources.</p> <p>This will download nltk stopwords if necessary, and merge all input lists into a single, sorted list without duplicates.</p> Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>class AssembleStopwordsModule(KiaraModule):\n\"\"\"Create a list of stopwords from one or multiple sources.\n\n    This will download nltk stopwords if necessary, and merge all input lists into a single, sorted list without duplicates.\n    \"\"\"\n\n    _module_type_name = \"create.stopwords_list\"\n\n    def create_inputs_schema(\n        self,\n    ) -&gt; ValueSetSchema:\n\n        return {\n            \"languages\": {\n                \"type\": \"list\",\n                \"doc\": \"A list of languages, will be used to retrieve language-specific stopword from nltk.\",\n                \"optional\": True,\n            },\n            \"stopword_list\": {\n                \"type\": \"list\",\n                \"doc\": \"A list of additional, custom stopwords.\",\n                \"optional\": True,\n            },\n        }\n\n    def create_outputs_schema(\n        self,\n    ) -&gt; ValueSetSchema:\n\n        return {\n            \"stopwords_list\": {\n                \"type\": \"list\",\n                \"doc\": \"A sorted list of unique stopwords.\",\n            }\n        }\n\n    def process(self, inputs: ValueMap, outputs: ValueMap):\n\n        stopwords = set()\n        _languages = inputs.get_value_obj(\"languages\")\n\n        if _languages.is_set:\n            all_stopwords = get_stopwords()\n            languages: ListModel = _languages.data\n\n            for language in languages.list_data:\n\n                if language not in all_stopwords.fileids():\n                    raise KiaraProcessingException(\n                        f\"Invalid language: {language}. Available: {', '.join(all_stopwords.fileids())}.\"\n                    )\n                stopwords.update(get_stopwords().words(language))\n\n        _stopword_lists = inputs.get_value_obj(\"stopword_list\")\n        if _stopword_lists.is_set:\n            stopword_lists: ListModel = _stopword_lists.data\n            for stopword_list in stopword_lists.list_data:\n                if isinstance(stopword_list, str):\n                    stopwords.add(stopword_list)\n                else:\n                    stopwords.update(stopword_list)\n\n        outputs.set_value(\"stopwords_list\", sorted(stopwords))\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.AssembleStopwordsModule-functions","title":"Functions","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.AssembleStopwordsModule.create_inputs_schema","title":"<code>create_inputs_schema() -&gt; ValueSetSchema</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>def create_inputs_schema(\n    self,\n) -&gt; ValueSetSchema:\n\n    return {\n        \"languages\": {\n            \"type\": \"list\",\n            \"doc\": \"A list of languages, will be used to retrieve language-specific stopword from nltk.\",\n            \"optional\": True,\n        },\n        \"stopword_list\": {\n            \"type\": \"list\",\n            \"doc\": \"A list of additional, custom stopwords.\",\n            \"optional\": True,\n        },\n    }\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.AssembleStopwordsModule.create_outputs_schema","title":"<code>create_outputs_schema() -&gt; ValueSetSchema</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>def create_outputs_schema(\n    self,\n) -&gt; ValueSetSchema:\n\n    return {\n        \"stopwords_list\": {\n            \"type\": \"list\",\n            \"doc\": \"A sorted list of unique stopwords.\",\n        }\n    }\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.AssembleStopwordsModule.process","title":"<code>process(inputs: ValueMap, outputs: ValueMap)</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>def process(self, inputs: ValueMap, outputs: ValueMap):\n\n    stopwords = set()\n    _languages = inputs.get_value_obj(\"languages\")\n\n    if _languages.is_set:\n        all_stopwords = get_stopwords()\n        languages: ListModel = _languages.data\n\n        for language in languages.list_data:\n\n            if language not in all_stopwords.fileids():\n                raise KiaraProcessingException(\n                    f\"Invalid language: {language}. Available: {', '.join(all_stopwords.fileids())}.\"\n                )\n            stopwords.update(get_stopwords().words(language))\n\n    _stopword_lists = inputs.get_value_obj(\"stopword_list\")\n    if _stopword_lists.is_set:\n        stopword_lists: ListModel = _stopword_lists.data\n        for stopword_list in stopword_lists.list_data:\n            if isinstance(stopword_list, str):\n                stopwords.add(stopword_list)\n            else:\n                stopwords.update(stopword_list)\n\n    outputs.set_value(\"stopwords_list\", sorted(stopwords))\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.RemoveStopwordsModule","title":"<code>RemoveStopwordsModule</code>","text":"<p>         Bases: <code>KiaraModule</code></p> <p>Remove stopwords from an array of token-lists.</p> Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>class RemoveStopwordsModule(KiaraModule):\n\"\"\"Remove stopwords from an array of token-lists.\"\"\"\n\n    _module_type_name = \"remove_stopwords.from.tokens_array\"\n\n    def create_inputs_schema(\n        self,\n    ) -&gt; ValueSetSchema:\n\n        # TODO: do something smart and check whether languages are already downloaded, if so, display selection in doc\n        inputs: Dict[str, Dict[str, Any]] = {\n            \"tokens_array\": {\n                \"type\": \"array\",\n                \"doc\": \"An array of string lists (a list of tokens).\",\n            },\n            \"languages\": {\n                \"type\": \"list\",\n                # \"doc\": f\"A list of language names to use default stopword lists for. Available: {', '.join(get_stopwords().fileids())}.\",\n                \"doc\": \"A list of language names to use default stopword lists for.\",\n                \"optional\": True,\n            },\n            \"additional_stopwords\": {\n                \"type\": \"list\",\n                \"doc\": \"A list of additional, custom stopwords.\",\n                \"optional\": True,\n            },\n        }\n        return inputs\n\n    def create_outputs_schema(\n        self,\n    ) -&gt; ValueSetSchema:\n\n        outputs = {\n            \"tokens_array\": {\n                \"type\": \"array\",\n                \"doc\": \"An array of string lists, with the stopwords removed.\",\n            }\n        }\n        return outputs\n\n    def process(self, inputs: ValueMap, outputs: ValueMap) -&gt; None:\n\n        import pyarrow as pa\n\n        custom_stopwords = inputs.get_value_data(\"additional_stopwords\")\n\n        if inputs.get_value_obj(\"languages\").is_set:\n            _languages: ListModel = inputs.get_value_data(\"languages\")\n            languages = _languages.list_data\n        else:\n            languages = []\n\n        stopwords = set()\n        if languages:\n            for language in languages:\n                if language not in get_stopwords().fileids():\n                    raise KiaraProcessingException(\n                        f\"Invalid language: {language}. Available: {', '.join(get_stopwords().fileids())}.\"\n                    )\n                stopwords.update(get_stopwords().words(language))\n\n        if custom_stopwords:\n            stopwords.update(custom_stopwords)\n\n        orig_array = inputs.get_value_obj(\"tokens_array\")  # type: ignore\n\n        if not stopwords:\n            outputs.set_value(\"tokens_array\", orig_array)\n            return\n\n        # if hasattr(orig_array, \"to_pylist\"):\n        #     token_lists = orig_array.to_pylist()\n\n        tokens_array = orig_array.data.arrow_array\n\n        # TODO: use vaex for this\n        result = []\n        for token_list in tokens_array:\n\n            cleaned_list = [x for x in token_list.as_py() if x.lower() not in stopwords]\n            result.append(cleaned_list)\n\n        outputs.set_value(\"tokens_array\", pa.chunked_array(pa.array(result)))\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.RemoveStopwordsModule-functions","title":"Functions","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.RemoveStopwordsModule.create_inputs_schema","title":"<code>create_inputs_schema() -&gt; ValueSetSchema</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>def create_inputs_schema(\n    self,\n) -&gt; ValueSetSchema:\n\n    # TODO: do something smart and check whether languages are already downloaded, if so, display selection in doc\n    inputs: Dict[str, Dict[str, Any]] = {\n        \"tokens_array\": {\n            \"type\": \"array\",\n            \"doc\": \"An array of string lists (a list of tokens).\",\n        },\n        \"languages\": {\n            \"type\": \"list\",\n            # \"doc\": f\"A list of language names to use default stopword lists for. Available: {', '.join(get_stopwords().fileids())}.\",\n            \"doc\": \"A list of language names to use default stopword lists for.\",\n            \"optional\": True,\n        },\n        \"additional_stopwords\": {\n            \"type\": \"list\",\n            \"doc\": \"A list of additional, custom stopwords.\",\n            \"optional\": True,\n        },\n    }\n    return inputs\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.RemoveStopwordsModule.create_outputs_schema","title":"<code>create_outputs_schema() -&gt; ValueSetSchema</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>def create_outputs_schema(\n    self,\n) -&gt; ValueSetSchema:\n\n    outputs = {\n        \"tokens_array\": {\n            \"type\": \"array\",\n            \"doc\": \"An array of string lists, with the stopwords removed.\",\n        }\n    }\n    return outputs\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.RemoveStopwordsModule.process","title":"<code>process(inputs: ValueMap, outputs: ValueMap) -&gt; None</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>def process(self, inputs: ValueMap, outputs: ValueMap) -&gt; None:\n\n    import pyarrow as pa\n\n    custom_stopwords = inputs.get_value_data(\"additional_stopwords\")\n\n    if inputs.get_value_obj(\"languages\").is_set:\n        _languages: ListModel = inputs.get_value_data(\"languages\")\n        languages = _languages.list_data\n    else:\n        languages = []\n\n    stopwords = set()\n    if languages:\n        for language in languages:\n            if language not in get_stopwords().fileids():\n                raise KiaraProcessingException(\n                    f\"Invalid language: {language}. Available: {', '.join(get_stopwords().fileids())}.\"\n                )\n            stopwords.update(get_stopwords().words(language))\n\n    if custom_stopwords:\n        stopwords.update(custom_stopwords)\n\n    orig_array = inputs.get_value_obj(\"tokens_array\")  # type: ignore\n\n    if not stopwords:\n        outputs.set_value(\"tokens_array\", orig_array)\n        return\n\n    # if hasattr(orig_array, \"to_pylist\"):\n    #     token_lists = orig_array.to_pylist()\n\n    tokens_array = orig_array.data.arrow_array\n\n    # TODO: use vaex for this\n    result = []\n    for token_list in tokens_array:\n\n        cleaned_list = [x for x in token_list.as_py() if x.lower() not in stopwords]\n        result.append(cleaned_list)\n\n    outputs.set_value(\"tokens_array\", pa.chunked_array(pa.array(result)))\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.PreprocessModule","title":"<code>PreprocessModule</code>","text":"<p>         Bases: <code>KiaraModule</code></p> <p>Preprocess lists of tokens, incl. lowercasing, remove special characers, etc.</p> <p>Lowercasing: Lowercase the words. This operation is a double-edged sword. It can be effective at yielding potentially better results in the case of relatively small datasets or datatsets with a high percentage of OCR mistakes. For instance, if lowercasing is not performed, the algorithm will treat USA, Usa, usa, UsA, uSA, etc. as distinct tokens, even though they may all refer to the same entity. On the other hand, if the dataset does not contain such OCR mistakes, then it may become difficult to distinguish between homonyms and make interpreting the topics much harder.</p> <p>Removing stopwords and words with less than three characters: Remove low information words. These are typically words such as articles, pronouns, prepositions, conjunctions, etc. which are not semantically salient. There are numerous stopword lists available for many, though not all, languages which can be easily adapted to the individual researcher's needs. Removing words with less than three characters may additionally remove many OCR mistakes. Both these operations have the dual advantage of yielding more reliable results while reducing the size of the dataset, thus in turn reducing the required processing power. This step can therefore hardly be considered optional in TM.</p> <p>Noise removal: Remove elements such as punctuation marks, special characters, numbers, html formatting, etc. This operation is again concerned with removing elements that may not be relevant to the text analysis and in fact interfere with it. Depending on the dataset and research question, this operation can become essential.</p> Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>class PreprocessModule(KiaraModule):\n\"\"\"Preprocess lists of tokens, incl. lowercasing, remove special characers, etc.\n\n    Lowercasing: Lowercase the words. This operation is a double-edged sword. It can be effective at yielding potentially better results in the case of relatively small datasets or datatsets with a high percentage of OCR mistakes. For instance, if lowercasing is not performed, the algorithm will treat USA, Usa, usa, UsA, uSA, etc. as distinct tokens, even though they may all refer to the same entity. On the other hand, if the dataset does not contain such OCR mistakes, then it may become difficult to distinguish between homonyms and make interpreting the topics much harder.\n\n    Removing stopwords and words with less than three characters: Remove low information words. These are typically words such as articles, pronouns, prepositions, conjunctions, etc. which are not semantically salient. There are numerous stopword lists available for many, though not all, languages which can be easily adapted to the individual researcher's needs. Removing words with less than three characters may additionally remove many OCR mistakes. Both these operations have the dual advantage of yielding more reliable results while reducing the size of the dataset, thus in turn reducing the required processing power. This step can therefore hardly be considered optional in TM.\n\n    Noise removal: Remove elements such as punctuation marks, special characters, numbers, html formatting, etc. This operation is again concerned with removing elements that may not be relevant to the text analysis and in fact interfere with it. Depending on the dataset and research question, this operation can become essential.\n    \"\"\"\n\n    _module_type_name = \"preprocess.tokens_array\"\n\n    KIARA_METADATA = {\n        \"tags\": [\"tokens\", \"preprocess\"],\n    }\n\n    def create_inputs_schema(\n        self,\n    ) -&gt; ValueSetSchema:\n\n        return {\n            \"tokens_array\": {\n                \"type\": \"array\",\n                \"doc\": \"The tokens array to pre-process.\",\n            },\n            \"to_lowercase\": {\n                \"type\": \"boolean\",\n                \"doc\": \"Apply lowercasing to the text.\",\n                \"default\": False,\n            },\n            \"remove_alphanumeric\": {\n                \"type\": \"boolean\",\n                \"doc\": \"Remove all tokens that include numbers (e.g. ex1ample).\",\n                \"default\": False,\n            },\n            \"remove_non_alpha\": {\n                \"type\": \"boolean\",\n                \"doc\": \"Remove all tokens that include punctuation and numbers (e.g. ex1a.mple).\",\n                \"default\": False,\n            },\n            \"remove_all_numeric\": {\n                \"type\": \"boolean\",\n                \"doc\": \"Remove all tokens that contain numbers only (e.g. 876).\",\n                \"default\": False,\n            },\n            \"remove_short_tokens\": {\n                \"type\": \"integer\",\n                \"doc\": \"Remove tokens shorter or equal to this value. If value is &lt;= 0, no filtering will be done.\",\n                \"default\": 0,\n            },\n            \"remove_stopwords\": {\n                \"type\": \"list\",\n                \"doc\": \"Remove stopwords.\",\n                \"optional\": True,\n            },\n        }\n\n    def create_outputs_schema(\n        self,\n    ) -&gt; ValueSetSchema:\n\n        return {\n            \"tokens_array\": {\n                \"type\": \"array\",\n                \"doc\": \"The pre-processed content, as an array of lists of strings.\",\n            }\n        }\n\n    def process(self, inputs: ValueMap, outputs: ValueMap):\n\n        import polars as pl\n        import pyarrow as pa\n\n        tokens_array: KiaraArray = inputs.get_value_data(\"tokens_array\")\n        lowercase: bool = inputs.get_value_data(\"to_lowercase\")\n        remove_alphanumeric: bool = inputs.get_value_data(\"remove_alphanumeric\")\n        remove_non_alpha: bool = inputs.get_value_data(\"remove_non_alpha\")\n        remove_all_numeric: bool = inputs.get_value_data(\"remove_all_numeric\")\n        remove_short_tokens: int = inputs.get_value_data(\"remove_short_tokens\")\n\n        if remove_short_tokens is None:\n            remove_short_tokens = -1\n\n        _remove_stopwords = inputs.get_value_obj(\"remove_stopwords\")\n        if _remove_stopwords.is_set:\n            stopword_list: Union[Iterable[str], None] = _remove_stopwords.data.list_data\n        else:\n            stopword_list = None\n\n        # it's better to have one method every token goes through, then do every test seperately for the token list\n        # because that way each token only needs to be touched once (which is more effective)\n        def check_token(token: str) -&gt; Union[str, None]:\n\n            # remove short tokens first, since we can save ourselves all the other checks (which are more expensive)\n            assert isinstance(remove_short_tokens, int)\n            if remove_short_tokens &gt; 0:\n                if len(token) &lt;= remove_short_tokens:\n                    return None\n\n            _token: str = token\n            if lowercase:\n                _token = _token.lower()\n\n            if remove_non_alpha:\n                match = _token if _token.isalpha() else None\n                if match is None:\n                    return None\n\n            # if remove_non_alpha was set, we don't need to worry about tokens that include numbers, since they are already filtered out\n            if remove_alphanumeric and not remove_non_alpha:\n                match = _token if _token.isalnum() else None\n                if match is None:\n                    return None\n\n            # all-number tokens are already filtered out if the remove_non_alpha methods above ran\n            if remove_all_numeric and not remove_non_alpha:\n                match = None if _token.isdigit() else _token\n                if match is None:\n                    return None\n\n            if stopword_list and _token and _token.lower() in stopword_list:\n                return None\n\n            return _token\n\n        series = pl.Series(name=\"tokens\", values=tokens_array.arrow_array)\n        result = series.apply(\n            lambda token_list: [\n                x for x in (check_token(token) for token in token_list) if x is not None\n            ]\n        )\n        result_array = result.to_arrow()\n\n        # TODO: remove this cast once the array data type can handle non-chunked arrays\n        chunked = pa.chunked_array(result_array)\n        outputs.set_values(tokens_array=chunked)\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.PreprocessModule-attributes","title":"Attributes","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.PreprocessModule.KIARA_METADATA","title":"<code>KIARA_METADATA = {'tags': ['tokens', 'preprocess']}</code>  <code>class-attribute</code>","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.PreprocessModule-functions","title":"Functions","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.PreprocessModule.create_inputs_schema","title":"<code>create_inputs_schema() -&gt; ValueSetSchema</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>def create_inputs_schema(\n    self,\n) -&gt; ValueSetSchema:\n\n    return {\n        \"tokens_array\": {\n            \"type\": \"array\",\n            \"doc\": \"The tokens array to pre-process.\",\n        },\n        \"to_lowercase\": {\n            \"type\": \"boolean\",\n            \"doc\": \"Apply lowercasing to the text.\",\n            \"default\": False,\n        },\n        \"remove_alphanumeric\": {\n            \"type\": \"boolean\",\n            \"doc\": \"Remove all tokens that include numbers (e.g. ex1ample).\",\n            \"default\": False,\n        },\n        \"remove_non_alpha\": {\n            \"type\": \"boolean\",\n            \"doc\": \"Remove all tokens that include punctuation and numbers (e.g. ex1a.mple).\",\n            \"default\": False,\n        },\n        \"remove_all_numeric\": {\n            \"type\": \"boolean\",\n            \"doc\": \"Remove all tokens that contain numbers only (e.g. 876).\",\n            \"default\": False,\n        },\n        \"remove_short_tokens\": {\n            \"type\": \"integer\",\n            \"doc\": \"Remove tokens shorter or equal to this value. If value is &lt;= 0, no filtering will be done.\",\n            \"default\": 0,\n        },\n        \"remove_stopwords\": {\n            \"type\": \"list\",\n            \"doc\": \"Remove stopwords.\",\n            \"optional\": True,\n        },\n    }\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.PreprocessModule.create_outputs_schema","title":"<code>create_outputs_schema() -&gt; ValueSetSchema</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>def create_outputs_schema(\n    self,\n) -&gt; ValueSetSchema:\n\n    return {\n        \"tokens_array\": {\n            \"type\": \"array\",\n            \"doc\": \"The pre-processed content, as an array of lists of strings.\",\n        }\n    }\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.PreprocessModule.process","title":"<code>process(inputs: ValueMap, outputs: ValueMap)</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>def process(self, inputs: ValueMap, outputs: ValueMap):\n\n    import polars as pl\n    import pyarrow as pa\n\n    tokens_array: KiaraArray = inputs.get_value_data(\"tokens_array\")\n    lowercase: bool = inputs.get_value_data(\"to_lowercase\")\n    remove_alphanumeric: bool = inputs.get_value_data(\"remove_alphanumeric\")\n    remove_non_alpha: bool = inputs.get_value_data(\"remove_non_alpha\")\n    remove_all_numeric: bool = inputs.get_value_data(\"remove_all_numeric\")\n    remove_short_tokens: int = inputs.get_value_data(\"remove_short_tokens\")\n\n    if remove_short_tokens is None:\n        remove_short_tokens = -1\n\n    _remove_stopwords = inputs.get_value_obj(\"remove_stopwords\")\n    if _remove_stopwords.is_set:\n        stopword_list: Union[Iterable[str], None] = _remove_stopwords.data.list_data\n    else:\n        stopword_list = None\n\n    # it's better to have one method every token goes through, then do every test seperately for the token list\n    # because that way each token only needs to be touched once (which is more effective)\n    def check_token(token: str) -&gt; Union[str, None]:\n\n        # remove short tokens first, since we can save ourselves all the other checks (which are more expensive)\n        assert isinstance(remove_short_tokens, int)\n        if remove_short_tokens &gt; 0:\n            if len(token) &lt;= remove_short_tokens:\n                return None\n\n        _token: str = token\n        if lowercase:\n            _token = _token.lower()\n\n        if remove_non_alpha:\n            match = _token if _token.isalpha() else None\n            if match is None:\n                return None\n\n        # if remove_non_alpha was set, we don't need to worry about tokens that include numbers, since they are already filtered out\n        if remove_alphanumeric and not remove_non_alpha:\n            match = _token if _token.isalnum() else None\n            if match is None:\n                return None\n\n        # all-number tokens are already filtered out if the remove_non_alpha methods above ran\n        if remove_all_numeric and not remove_non_alpha:\n            match = None if _token.isdigit() else _token\n            if match is None:\n                return None\n\n        if stopword_list and _token and _token.lower() in stopword_list:\n            return None\n\n        return _token\n\n    series = pl.Series(name=\"tokens\", values=tokens_array.arrow_array)\n    result = series.apply(\n        lambda token_list: [\n            x for x in (check_token(token) for token in token_list) if x is not None\n        ]\n    )\n    result_array = result.to_arrow()\n\n    # TODO: remove this cast once the array data type can handle non-chunked arrays\n    chunked = pa.chunked_array(result_array)\n    outputs.set_values(tokens_array=chunked)\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens-functions","title":"Functions","text":""},{"location":"reference/kiara_plugin/language_processing/modules/tokens/#kiara_plugin.language_processing.modules.tokens.get_stopwords","title":"<code>get_stopwords()</code>","text":"Source code in <code>kiara_plugin/language_processing/modules/tokens.py</code> <pre><code>def get_stopwords():\n\n    # TODO: make that smarter\n    import nltk\n\n    output = io.StringIO()\n    nltk.download(\"punkt\", print_error_to=output)\n    nltk.download(\"stopwords\", print_error_to=output)\n\n    log.debug(\"external.message\", source=\"nltk\", msg=output.getvalue())\n    from nltk.corpus import stopwords\n\n    return stopwords\n</code></pre>"},{"location":"reference/kiara_plugin/language_processing/pipelines/__init__/","title":"pipelines","text":"<p>Default (empty) module that is used as a base path for pipelines contained in this package.</p>"}]}